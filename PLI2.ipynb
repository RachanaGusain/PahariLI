{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "PLI2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachanaGusain/PahariLI/blob/main/PLI2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnubs-Nnb3cw"
      },
      "source": [
        "# Language Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoUDvx56cWhQ"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvVIhx9wppqB",
        "outputId": "eb0124a1-5018-4eb4-bb9e-6a59550f192a"
      },
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling scikit-learn-0.22.2.post1:\n",
            "  Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 126kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOXkvuQk_JuF"
      },
      "source": [
        "import os\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from time import time\n",
        "from scipy import sparse\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.manifold import TSNE, MDS\n",
        "from collections import namedtuple, defaultdict, Counter, OrderedDict\n",
        "from itertools import tee, islice, accumulate, combinations\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCR2dz2q7XiJ"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use(\"pgf\")\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"pgf.texsystem\": 'pdflatex',\n",
        "    \"font.family\": 'serif',  # use serif/main font for text elements\n",
        "    \"text.usetex\": True,     # use inline math for ticks\n",
        "    \"pgf.rcfonts\": False,    # don't setup fonts from rc parameters\n",
        "    \"font.size\": 8,          # Use 8pt font in plots, to match 10pt font in document\n",
        "    \"axes.titlesize\": 8,\n",
        "    \"axes.labelsize\": 8,\n",
        "    \"xtick.labelsize\": 6,    # Make the legend/label fonts a little smaller\n",
        "    \"ytick.labelsize\": 6,\n",
        "    \"xtick.major.size\": 0,\n",
        "    \"ytick.major.size\": 0,\n",
        "    \"xtick.major.width\": 0.2,\n",
        "    \"ytick.major.width\": 0.2,\n",
        "    \"xtick.minor.size\" : 1.5,\n",
        "    \"xtick.minor.width\": 0.2,\n",
        "    \"xtick.direction\": 'in',\n",
        "    \"lines.markersize\": 1.2,\n",
        "    \"lines.linewidth\": 0.5,\n",
        "    \"hatch.linewidth\": 0.4,\n",
        "    \"patch.linewidth\": 0.2,\n",
        "    \"axes.prop_cycle\": matplotlib.cycler('color', 'k'),\n",
        "    \"hatch.color\": 'k',\n",
        "    \"axes.linewidth\": 0.2,\n",
        "    \"grid.linewidth\": 0.2,\n",
        "    \"legend.fontsize\": 6,\n",
        "    \"legend.title_fontsize\": 6,\n",
        "    \"legend.labelspacing\": 0.1,\n",
        "    \"legend.handlelength\": 3,\n",
        "    \"legend.frameon\": False,\n",
        "    \"savefig.dpi\": 1000,\n",
        "    \"savefig.bbox\": 'tight',\n",
        "    \"savefig.format\": 'pdf'\n",
        "    })\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBx2NeyG4vxM"
      },
      "source": [
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt install texlive-latex-extra\n",
        "! sudo apt install dvipng\n",
        "! sudo apt install cm-super\n",
        "#!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rkhLLrFLfM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3s3ZOK18oM2"
      },
      "source": [
        "num_clf = 2\n",
        "clflist = [\"mnb\", \"svm\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYqCIxzFb3c5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q02Yj6oGxZB_"
      },
      "source": [
        "# Data files\n",
        "tr_file = \"/content/drive/MyDrive/PLI/data/train.txt\"\n",
        "ts_file = \"/content/drive/MyDrive/PLI/data/test.txt\"\n",
        "\n",
        "# Load data\n",
        "tr_data = open(tr_file, mode='r', encoding='utf-8')\n",
        "ts_data = open(ts_file, mode='r', encoding='utf-8')\n",
        "\n",
        "print(\"Data loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWeG0HRpxZB_"
      },
      "source": [
        "# Separate text and labels\n",
        "tr_text = []\n",
        "tr_lang = []\n",
        "ts_text = []\n",
        "ts_lang = []\n",
        "\n",
        "for line in tr_data:\n",
        "    text, lang = line.strip().split('\\t')\n",
        "    tr_text.append(text)\n",
        "    tr_lang.append(lang)\n",
        "\n",
        "for line in ts_data:\n",
        "    text, lang = line.strip().split('\\t')\n",
        "    ts_text.append(text)\n",
        "    ts_lang.append(lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YE576k_wyOi"
      },
      "source": [
        "label = {'dgo': 0, 'gbm': 1, 'kfy': 2, 'npi': 3}\n",
        "\n",
        "y_tr = np.asarray(list(map(lambda x: label[x], tr_lang)))\n",
        "y_ts = np.asarray(list(map(lambda x: label[x], ts_lang)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52MEhE75dhve"
      },
      "source": [
        "def size_mb(docs):\n",
        "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
        "\n",
        "tr_size_mb = size_mb(tr_text)\n",
        "ts_size_mb = size_mb(ts_text)\n",
        "\n",
        "print(\"Train data: %d sentences - %0.2f MB\" % (len(tr_text), tr_size_mb))\n",
        "print(\"Test data : %d sentences - %0.2f MB\" % (len(ts_text), ts_size_mb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxYmsfwyVI79"
      },
      "source": [
        "## Language Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNyCTAQIizqV"
      },
      "source": [
        "def ngrams(term, ngram_range, min_df=1, max_df=1.0):\n",
        "    \"\"\"\n",
        "    Function to extract word or char n-gram features.\n",
        "\n",
        "    Parameters:\n",
        "        analyzer: string {'word', 'char', 'char_wb'}\n",
        "            Whether the feature should be made of word n-gram or character n-grams.\n",
        "            Option 'char_wb' creates character n-grams only from text inside word boundaries;\n",
        "            n-grams at the edges of words are padded with space.\n",
        "\n",
        "        ngram_range: tuple (min_n, max_n), default=(1, 1)\n",
        "            The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
        "            All values of n such that min_n <= n <= max_n will be used.\n",
        "            For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
        "\n",
        "        min_df: float in range [0.0, 1.0] or int, default=1\n",
        "            When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "            This value is also called cut-off in the literature.\n",
        "            If float, the parameter represents a proportion of documents, integer absolute counts.\n",
        "\n",
        "        max_df: float in range [0.0, 1.0] or int, default=1.0\n",
        "            When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
        "            If float, the parameter represents a proportion of documents, integer absolute counts.\n",
        "\n",
        "    Returns:\n",
        "        z: dict {features: list of features, tr_analyzer_ngram: scipy.sparse.csr.csr_matrix, ts_analyzer_ngram: scipy.sparse.csr.csr_matrix}\n",
        "    \"\"\"\n",
        "    \n",
        "    i, j = ngram_range\n",
        "    \n",
        "    def word_ngram_analyzer(doc):\n",
        "        for line in doc.split('\\n'):\n",
        "            terms = re.findall(r\"\\w+\", line)\n",
        "            for n in range(i, j+1):\n",
        "                #for ngram in zip(*[terms[k:] for k in range(n)]): #solution without a generator (works the same but has higher memory usage)\n",
        "                for ngram in zip(*[islice(seq, k, len(terms)) for k, seq in enumerate(tee(terms, n))]): # solution using a generator\n",
        "                    ngram = \" \".join(ngram)\n",
        "                    yield ngram\n",
        "    \n",
        "    try:\n",
        "        if term == 'word':      # CountVectorizer gives improper word n-grams for Devanagari script\n",
        "            vectorizer = CountVectorizer(analyzer=word_ngram_analyzer, min_df=min_df, max_df=max_df)\n",
        "        else:\n",
        "            vectorizer = CountVectorizer(analyzer=term, ngram_range=(i, j), min_df=min_df, max_df=max_df)\n",
        "        vectorizer.fit(tr_text)# Learn the vocabulary using train data\n",
        "    except ValueError:\n",
        "        print(\"Error: After pruning, no terms remain.\")\n",
        "        return None\n",
        "\n",
        "    # Store the extracted features and vectorized text in a dictionary\n",
        "    z = dict()\n",
        "    z[\"features\"] = vectorizer.get_feature_names()\n",
        "\n",
        "    # Use the learned vocabulary to vectorize train and test text data\n",
        "    z[\"tr_\"+term+\"_\"+str(i)+str(j)] = vectorizer.transform(tr_text)\n",
        "    z[\"ts_\"+term+\"_\"+str(i)+str(j)] = vectorizer.transform(ts_text)\n",
        "\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ILlhY8Ykph"
      },
      "source": [
        "def classifier(clf, params, X_tr, y_tr, X_ts, y_ts):\n",
        "    print(\"Training\", clf)\n",
        "\n",
        "    search = GridSearchCV(estimator=clf, param_grid=params, scoring='accuracy', \n",
        "                          cv=5, verbose=1, return_train_score=True)\n",
        "    t0 = time()\n",
        "    search.fit(X_tr, y_tr)\n",
        "    tr_val_time = time() - t0\n",
        "    clf = search.best_estimator_\n",
        "    print(\"\\nBest Estimator:\", clf)\n",
        "    print(\"\\nTrain and validation time: %.4f seconds\" % tr_val_time)\n",
        "\n",
        "    t0 = time()\n",
        "    y_true, y_pred = y_ts, clf.predict(X_ts)\n",
        "    ts_time = time() - t0\n",
        "    print(\"\\nTest time: %.4f seconds\" % ts_time)\n",
        "    print(\"\\nTest set accuracy: %.4f\" \n",
        "          % clf.score(X_ts, y_ts))        # mean accuracy on the given test data\n",
        "\n",
        "    confusion_mat = metrics.confusion_matrix(y_true, y_pred)\n",
        "    scores_report = metrics.classification_report(y_true, y_pred, target_names=label.keys(), output_dict=True)\n",
        "    scores_report = pd.DataFrame(scores_report)           # convert to dataframe\n",
        "    \n",
        "    print(\"\\nClassification report:\")\n",
        "    # we can print/ pretty print the dataframe but the text report looks better\n",
        "    print(metrics.classification_report(y_true, y_pred, target_names=label.keys(), digits=4))\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    cm_disp = metrics.plot_confusion_matrix(clf, X_ts, y_ts, values_format='d', \n",
        "                                            display_labels=label.keys(),\n",
        "                                            cmap=plt.cm.Blues, colorbar=False)\n",
        "    plt.show()\n",
        "\n",
        "    return search.best_params_, tr_val_time, ts_time, scores_report, cm_disp, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMVKEZ-S9SkC"
      },
      "source": [
        "def build_ngram_model(max_n):\n",
        "    ngram = range(1, max_n+1)\n",
        "    ngram_range = [(i, j) for i in ngram for j in ngram if i<=j]\n",
        "\n",
        "    outputs = {clf: dict() for clf in clflist}  # to store predicted target values\n",
        "    results = [[] for _ in range(num_clf)]  # [[]]*num_clf gives a list where each element references to the same object\n",
        "    results = {clf: [] for clf in clflist}  # for access by clf name\n",
        "    columns = [\"Vectorizer\", \"#Features\", \"Hyperparameter\", \n",
        "               \"Train&Val time (s)\", \"Test time (s)\", \n",
        "               \"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"]\n",
        "\n",
        "    for analyzer in ['word', 'char', 'char_wb']:\n",
        "        for (i, j) in ngram_range:\n",
        "            # Extract n-gram features for train and test data\n",
        "            print('*'*80)\n",
        "            print(f\"Extracting frequency based {analyzer} n-gram features...\")\n",
        "            z = ngrams(term=analyzer, ngram_range=(i, j), min_df=0.005)\n",
        "            if z is None:   # Check if no ngrams for the specified range exist             \n",
        "                continue\n",
        "            print(\"First 10 features (not by frequency):\", z[\"features\"][:10])\n",
        "\n",
        "            vect = analyzer+\"_\"+str(i)+str(j)\n",
        "            X_tr = z[\"tr_\"+vect].toarray()\n",
        "            X_ts = z[\"ts_\"+vect].toarray()\n",
        "\n",
        "            print(\"\\n{} ({},{})-gram vectorized text:\".format(analyzer, i, j))\n",
        "            print(\"Train:\", X_tr.shape)\n",
        "            print(\"Test :\", X_ts.shape)\n",
        "\n",
        "            # Train classifier models     \n",
        "            for (clf, params, descript), clfname in zip([\n",
        "                    (MultinomialNB(), \n",
        "                     {'alpha': np.power(10, np.arange(-3, 2, dtype=float))}, \n",
        "                     \"Multinomial Naïve Bayes Classifier\"),\n",
        "                    (LinearSVC(dual=False), \n",
        "                     {'C': np.power(10, np.arange(-3, 2, dtype=float))}, \n",
        "                     \"Linear Support Vector Classifier\")], \n",
        "                    clflist):\n",
        "                print('=' *80)\n",
        "                print(descript)\n",
        "                print('=' * 80)\n",
        "                best_param, tr_val_time, ts_time, scores, cm_disp, y_pred = classifier(clf, params, X_tr, y_tr, X_ts, y_ts)\n",
        "                \n",
        "                # Save the confusion matrix and its plot\n",
        "                cm_df = pd.DataFrame(cm_disp.confusion_matrix, index=label.keys(), columns=label.keys())\n",
        "                cm_df.to_csv(os.path.join(dirpath, 'confusion_matrix', 'cm_'+clfname+'_'+vect+'.csv'))\n",
        "                #cm_disp.figure_.savefig(os.path.join(dirpath, 'confusion_matrix', 'cm_'+clfname+'_'+vect+'.pdf'),\n",
        "                #                        bbox_inches='tight')\n",
        "                # Store result for the current configuration\n",
        "                result = []\n",
        "                result.extend([vect, len(z[\"features\"])])           # vectorizer and number of features\n",
        "                result.extend(best_param.values())                  # best hyper parameter\n",
        "                result.extend([tr_val_time, ts_time])               # train and test time\n",
        "                result.extend(scores[\"macro avg\"].tolist()[:-1])    # precision, recall and f1-score\n",
        "                result.append(scores[\"accuracy\"].iat[0]*100)        # accuracy\n",
        "                results[clfname].append(dict(zip(columns, result)))\n",
        "                # Store the predicted target values\n",
        "                outputs[clfname][vect] = y_pred\n",
        "\n",
        "    # convert list of dicts to dataframe\n",
        "    return {clf: pd.DataFrame(results[clf]) for clf in clflist}, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpcfqrgxtvBK"
      },
      "source": [
        "dirpath = \"/content/drive/MyDrive/PLI/results\"\n",
        "results, outputs = build_ngram_model(8)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}